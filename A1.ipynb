{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunbjoseph/MIE1517-A1/blob/main/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d3b2b1",
      "metadata": {
        "id": "e5d3b2b1"
      },
      "source": [
        "# Assignment 1: PyTorch Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ea05c8",
      "metadata": {
        "id": "f0ea05c8"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this assignment, you will:\n",
        "1. Build a simple neural network from scratch to understand how neural networks work.\n",
        "2. Train a neural network (using PyTorch) to classify images from the Garbage Classification dataset into one of 7 classes (battery, biological, cardboard, etc.).\n",
        "\n",
        "The provided code will guide you through key steps, but by the end of the assignment, you should:\n",
        "\n",
        "1. Understand the training loop for a machine learning model.\n",
        "2. Distinguish between training, validation, and test data.\n",
        "3. Learn about overfitting and underfitting.\n",
        "4. Explore how hyperparameters, like learning rate and batch size, impact training.\n",
        "5. Compare a basic feedforward neural network (ANN) with a convolutional neural network (CNN).\n",
        "\n",
        "This assignment is inspired by materials developed by Prof. Lisa Zhang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1264a4",
      "metadata": {
        "id": "bd1264a4"
      },
      "source": [
        "## What to Submit\n",
        "\n",
        "1. Submit the `.ipynb` and `.html` files containing your code, outputs, and answers from all parts. Please take extra effort to make your answers and submissions readable. Do not display unnecessary outputs, only the ones that are important for supporting your answers.\n",
        "\n",
        "  You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
        "\n",
        "\n",
        "2. Submit your model predictions on the secret test dataset for Parts B-5 and B-6  as `labels_part_b5.csv` and `labels_part_b6.csv`.\n",
        "\n",
        "**Important**: Do not submit additional files generated by your code.\n",
        "\n",
        "Please use Google Colab for this assignment. If you prefer Jupyter Notebook, ensure the file is uploaded to Colab for submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your colab file here and ensure the file can be accessed by the our teaching team.\n",
        "\n",
        "Colab Link:"
      ],
      "metadata": {
        "id": "Eq-3vRF1iBtJ"
      },
      "id": "Eq-3vRF1iBtJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "https://github.com/tarunbjoseph/MIE1517-A1/blob/main/A1.ipynb"
      ],
      "metadata": {
        "id": "L1XVyXbHh_yo"
      },
      "id": "L1XVyXbHh_yo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "26959fce",
      "metadata": {
        "id": "26959fce"
      },
      "source": [
        "# PART A: Building a Neural Network from Scratch [8pt]\n",
        "\n",
        "Before we get into using PyTorch to train our classifier we will go through the process of creating our neural network from scratch. We've seen in the tutorial how to build a 1-layer network, now we'll take it one step further to build a 2-layer network. This is an important exercise that everyone should attempt at least once to understand and truly appreciate the workings of neural networks.\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "To help guide the construction we will use the sklearn wine dataset. Provided are some helper code to get us started:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "import numpy as np\n",
        "\n",
        "d = load_wine()\n",
        "raw_data = np.c_[d.data, d.target]\n",
        "raw_data = raw_data[~np.isnan(raw_data).any(axis=1)].astype(float)\n",
        "\n",
        "np.random.shuffle(raw_data)"
      ],
      "metadata": {
        "id": "Zrh2cqiwih4J"
      },
      "id": "Zrh2cqiwih4J",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# split your data into training and validation\n",
        "X_train = raw_data[0:100,:13]\n",
        "y_train = raw_data[0:100,13:14].astype(int)\n",
        "X_val = raw_data[100:,:13]\n",
        "y_val = raw_data[100:,13:14].astype(int)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train.dtype, y_train.dtype)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_val.dtype, y_val.dtype)"
      ],
      "metadata": {
        "id": "aIx4d85dilZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfd7d81-01d2-460b-997a-c26ad25182de"
      },
      "id": "aIx4d85dilZW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 13) (100, 1)\n",
            "float64 int64\n",
            "(78, 13) (78, 1)\n",
            "float64 int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the neural network output consists of several nodes, one for each output class. Since the labels are provided as integers we will need to convert them into one-hot vectors to match the neural network output format."
      ],
      "metadata": {
        "id": "7AT57UtCinRc"
      },
      "id": "7AT57UtCinRc"
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert array to one-hot encoding\n",
        "def to_one_hot(Y):\n",
        "    n_col = np.amax(Y) + 1\n",
        "    binarized = np.zeros((len(Y), n_col))\n",
        "    for i in range(len(Y)):\n",
        "        binarized[i, Y[i]] = 1.\n",
        "    return binarized"
      ],
      "metadata": {
        "id": "5mBM-n0NinwY"
      },
      "id": "5mBM-n0NinwY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_one_hot(y_train)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train.dtype, y_train.dtype)\n",
        "\n",
        "y_val = to_one_hot(y_val)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_val.dtype, y_val.dtype)"
      ],
      "metadata": {
        "id": "XJTUUg2miqKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1a9268-d7d2-4b3f-c5ae-c291b0de4fad"
      },
      "id": "XJTUUg2miqKT",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 13) (100, 3)\n",
            "float64 float64\n",
            "(78, 13) (78, 3)\n",
            "float64 float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verify one-hot encoding\n",
        "y_train[0:5,:]"
      ],
      "metadata": {
        "id": "gkCLM_x2ir0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d1670e-9475-45cb-ff3e-83bacc10a3ba"
      },
      "id": "gkCLM_x2ir0N",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A-1. Develop a 2-layer ANN [6pt MODEL]\n",
        "At its core a 2-layer neural network is just a few lines of code. Most of the complexity comes from setting up the training of the network.\n",
        "\n",
        "Using vectorized form, set up the neural network training to use a cross-entropy loss function and determine the gradients with resepect to the layer 1 and layer 2 weights."
      ],
      "metadata": {
        "id": "5pHxfcR1iuU5"
      },
      "id": "5pHxfcR1iuU5"
    },
    {
      "cell_type": "code",
      "source": [
        "# write code to create a 2-layer ANN in vectorized form\n",
        "\n",
        "#define sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "#define softmax\n",
        "def softmax(x):\n",
        "  e = np.exp(x)\n",
        "  return e/e.sum(axis=1, keepdims = True)\n",
        "\n",
        "\n",
        "def ann(W, X_train, y_train):\n",
        "\n",
        "  num_hidden = 20\n",
        "  num_features = 13\n",
        "  num_outputs = 3\n",
        "\n",
        "  #Weights\n",
        "  w0 = W[:num_features*num_hidden].reshape(num_features, num_hidden)\n",
        "  w1 = W[num_features*num_hidden:].reshape(num_hidden, num_outputs)\n",
        "\n",
        "  #Feed forward\n",
        "  layer0 = X_train\n",
        "  layer1 = sigmoid(np.dot(layer0, w0))\n",
        "  layer2 = np.dot(layer1, w1)\n",
        "\n",
        "  # softmax\n",
        "  y_pred = softmax(layer2)\n",
        "\n",
        "  #Back propagation using gradient descent\n",
        "\n",
        "  #cross-entropy loss\n",
        "  error = -np.sum(y_train * np.log(y_pred)) / y_train.shape[0]\n",
        "\n",
        "  #initialize gradients to zero\n",
        "  dw0 = np.zeros_like(w0)\n",
        "  dw1 = np.zeros_like(w1)\n",
        "\n",
        "  #calculate gradients\n",
        "  # For the output layer, I'm dividing by the batch size in order to average the gradients across the batch for stable weight updates\n",
        "  delta2 = (y_pred - y_train) / y_train.shape[0]\n",
        "  delta1 = np.dot(delta2, w1.T) * layer1 * (1 - layer1)\n",
        "  # dw1 += np.dot(layer1.T, delta2)\n",
        "  # dw0 += np.dot(layer0.T, delta1)\n",
        "\n",
        "  #determine gradients\n",
        "  dw1 += np.dot(layer1.T, delta2)\n",
        "  dw0 += np.dot(layer0.T, delta1)\n",
        "\n",
        "  #combine gradients into one vector\n",
        "  dW = np.concatenate([dw0.flatten(), dw1.flatten()])\n",
        "\n",
        "  return (error, dW, y_pred)\n"
      ],
      "metadata": {
        "id": "WjTNDPBXixG5"
      },
      "id": "WjTNDPBXixG5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A-2. Train your neural network [1pt RESULT]\n",
        "Train your neural network once with random initialization (uniformly at random between -1 and +1), and once with zero initialization (all weights = 0).\n",
        "\n",
        "Compare results. Comment on how well does your network work on the wine dataset in these two cases and why?"
      ],
      "metadata": {
        "id": "Jk_z4tfmizB4"
      },
      "id": "Jk_z4tfmizB4"
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden = 20\n",
        "num_features = 13\n",
        "num_outputs = 3\n",
        "\n",
        "\n",
        "def train_and_evaluate(w0_init, w1_init, label=\"\"):\n",
        "\n",
        "    #initial weights: w0_init, w1_init\n",
        "\n",
        "    #combine weights into a single vector\n",
        "    W = np.array(list(w0_init.flatten()) + list(w1_init.flatten()))\n",
        "\n",
        "    #train network\n",
        "    n = 0.001\n",
        "    iterations = 100000\n",
        "    errors = []\n",
        "    for i in range(iterations):\n",
        "        (error, dW, y_pred) = ann(W, X_train, y_train)\n",
        "        W += -dW * n\n",
        "        errors.append(error)\n",
        "\n",
        "    # final predictions\n",
        "    _, _, y_pred = ann(W, X_train, y_train)\n",
        "    y_pred_labels = np.round(y_pred, 0)\n",
        "    y_true_labels = np.round(y_train, 0)\n",
        "\n",
        "    print(f\"{label} initialization: Predictions on training data: \\n{y_pred_labels[:5]}\")\n",
        "    print(f\"{label} initialization: Ground truth training data: \\n{y_true_labels[:5]}\")\n",
        "\n",
        "    accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_train, axis=1))\n",
        "    print(f\"{label} initialization: Final training accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return errors, accuracy"
      ],
      "metadata": {
        "id": "Lq30JMcdi1G7"
      },
      "id": "Lq30JMcdi1G7",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled random initialization\n",
        "w0_rand = np.random.randn(num_features, num_hidden) * 0.01\n",
        "w1_rand = np.random.randn(num_hidden, num_outputs) * 0.01\n",
        "errors_rand, accuracy_rand = train_and_evaluate(w0_rand, w1_rand, \"Random\")"
      ],
      "metadata": {
        "id": "pDDoqYici2zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb56753-2f45-4650-9b7d-f92ffb21d0e5"
      },
      "id": "pDDoqYici2zX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random initialization: Predictions on training data: \n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "Random initialization: Ground truth training data: \n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "Random initialization: Final training accuracy: 0.9500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uniform random initialization between -1 and +1\n",
        "w0_uniform = np.random.uniform(-1, 1, size=(num_features, num_hidden))\n",
        "w1_uniform = np.random.uniform(-1, 1, size=(num_hidden, num_outputs))\n",
        "errors_uniform, accuracy_uniform = train_and_evaluate(w0_uniform, w1_uniform, \"Uniform\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1jvH-KkDjF7",
        "outputId": "c383ea73-8938-4d8c-bfdb-60cd3b570b84"
      },
      "id": "m1jvH-KkDjF7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1312233640.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform initialization: Predictions on training data: \n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Uniform initialization: Ground truth training data: \n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "Uniform initialization: Final training accuracy: 0.4200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zero initialization\n",
        "w0_zero = np.zeros((num_features, num_hidden))\n",
        "w1_zero = np.zeros((num_hidden, num_outputs))\n",
        "errors_zero, acc_zero = train_and_evaluate(w0_zero, w1_zero, \"Zero\")"
      ],
      "metadata": {
        "id": "5i0XJqsoi4Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff2dd92-5f16-4d21-83fc-6526f45cd307"
      },
      "id": "5i0XJqsoi4Ft",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero initialization: Predictions on training data: \n",
            "[[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Zero initialization: Ground truth training data: \n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "Zero initialization: Final training accuracy: 0.3600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "Based on the three initialization results obtained above, I can see that:\n",
        "- Scaled Random Initialization has the highest accuracy (0.95).This method is generally effective as it breaks symmetry between neurons, allowing\n",
        "them to learn distinct features. The small values prevent activation functions (sigmoid and softmax) from saturating too early, thus avoiding\n",
        "vanishing/exploding gradient issues and ensuring a good fit.\n",
        "\n",
        "- Uniform Initialization between -1 and +1 resulted in an accuracy of 0.42. The large intial weights caused the sigmoid activation function to\n",
        "output values very close to 0 or 1, leading to saturation which resulted in vanishing gradients, making it difficult for the network to learn effectively.\n",
        "\n",
        "- Zero Initialization resulted in the lowest accuracy of 0.36. Initializing all weights to zero leads to a symmetry problem. All neurons in a layer\n",
        "will learn the exact same features during backpropagation, effectively making the network no more powerful than a single neuron.\n",
        "\n",
        "- In summary, I observed that proper random initialization is crucial in order for the network to learn effectively without issues like vanishing\n",
        "gradients or symmetry problems.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "LckcwazZi5Vy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "284951c8-454c-40b9-e411-71e92f6c1bdf"
      },
      "id": "LckcwazZi5Vy",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPROVIDE YOUR ANSWER BELOW\\n\\nBased on the three initialization results obtained above, I can see that:\\n- Scaled Random Initialization has the highest accuracy (0.95).This method is generally effective as it breaks symmetry between neurons, allowing\\nthem to learn distinct features. The small values prevent activation functions (sigmoid and softmax) from saturating too early, thus avoiding \\nvanishing/exploding gradient issues and ensuring a good fit.\\n\\n- Uniform Initialization between -1 and +1 resulted in an accuracy of 0.42. The large intial weights caused the sigmoid activation function to\\noutput values very close to 0 or 1, leading to saturation which resulted in vanishing gradients, making it difficult for the network to learn effectively.\\n\\n- Zero Initialization resulted in the lowest accuracy of 0.36. Initializing all weights to zero leads to a symmetry problem. All neurons in a layer \\nwill learn the exact same features during backpropagation, effectively making the network no more powerful than a single neuron.\\n\\n- In summary, I observed that proper random initialization is crucial in order for the network to learn effectively without issues like vanishing\\ngradients or symmetry problems.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part A-3. Validation [2pt MODEL]\n",
        "Validate that the gradients were computed correctly for the 2-layer neural network you developed."
      ],
      "metadata": {
        "id": "G55iNMrvi6_6"
      },
      "id": "G55iNMrvi6_6"
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden = 20\n",
        "num_features = 13\n",
        "num_outputs = 3\n",
        "\n",
        "#initialize weights uniformly at random between -1 and +1, and combine weights\n",
        "w0 = np.random.uniform(-1, 1, size=(num_features, num_hidden))\n",
        "w1 = np.random.uniform(-1, 1, size=(num_hidden, num_outputs))\n",
        "W = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "\n",
        "#compute gradients analytically\n",
        "(error, dW, y_pred) = ann(W, X_train, y_train)\n",
        "\n",
        "#compute gradients numerically\n",
        "dW_num = np.zeros((len(W),1))\n",
        "\n",
        "epsilon = 1e-4 # Define a small epsilon for numerical approximation\n",
        "\n",
        "for ind in range(len(W)):\n",
        "  #reset gradients\n",
        "  We1 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "  We2 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "\n",
        "  #increment slightly\n",
        "  We1[ind] = W[ind] - epsilon\n",
        "  We2[ind] = W[ind] + epsilon\n",
        "\n",
        "  #compute errors\n",
        "  (error_e1, dW_e1, y_pred1) = ann(We1, X_train, y_train)\n",
        "  (error_e2, dW_e2, y_pred2) = ann(We2, X_train, y_train)\n",
        "\n",
        "  #obtain numerical gradients\n",
        "  grad_num = (error_e2 - error_e1) / (We2[ind] - We1[ind])\n",
        "\n",
        "  #display difference between numerical and analytic gradients\n",
        "  print(round(abs(grad_num - dW[ind]), 4), grad_num, dW[ind])"
      ],
      "metadata": {
        "id": "oPEVZVHEi9iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2d15ce-8c71-4d21-d8df-d5ca264f1745"
      },
      "id": "oPEVZVHEi9iY",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 0.026879299074303493 0.02687930046718458\n",
            "0.0 0.0 5.605422008009281e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 1.3930023801122672e-06 1.3930008019004736e-06\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.03053570193589986 0.030535702190372643\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.0851711175498076 -0.08517112036527716\n",
            "0.0 0.0 2.8926087834479264e-69\n",
            "0.0 0.0 -1.635683590545376e-95\n",
            "0.0 0.0 1.0468368268626657e-53\n",
            "0.0 -1.4944079307356841e-06 -1.4944080914756307e-06\n",
            "0.0 0.0 4.02416739392474e-42\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.007045615262325166 -0.007045610267847479\n",
            "0.0 0.0 1.3542532428885018e-69\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.00752320038488546 0.007523200513792544\n",
            "0.0 0.0 4.297490573148081e-64\n",
            "0.0 0.0 0.0\n",
            "0.0 1.5515499995899073e-06 1.5515494864584916e-06\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.003636523021599959 0.003636523019261678\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.011808341087161858 -0.011808341052402012\n",
            "0.0 0.0 2.2176667339767807e-70\n",
            "0.0 0.0 -1.2540240860847887e-96\n",
            "0.0 0.0 8.025749006030987e-55\n",
            "0.0 -1.2893242029578287e-07 -1.289326059658023e-07\n",
            "0.0 0.0 3.0851950058912793e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.004900615631076561 -0.004900615593234712\n",
            "0.0 0.0 1.0382608195478548e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.004674562774643025 0.004674562780012644\n",
            "0.0 0.0 9.342370033971391e-64\n",
            "0.0 0.0 0.0\n",
            "0.0 6.53033183084589e-07 6.530331947278096e-07\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.005891679772940899 0.005891679767973126\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.0157653476129389 -0.015765347643036776\n",
            "0.0 0.0 4.821014639079879e-70\n",
            "0.0 0.0 -2.7261393175756267e-96\n",
            "0.0 0.0 1.7447280447719211e-54\n",
            "0.0 -2.5168866990557535e-07 -2.5168729867000236e-07\n",
            "0.0 0.0 6.706945656793554e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.001298502099178621 -0.0012985020694289721\n",
            "0.0 0.0 2.257088738147503e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.04551519685925211 0.04551520219679006\n",
            "0.0 0.0 8.875251541377115e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 4.598299518932487e-06 4.598290494442967e-06\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.05619042772520045 0.05619042471643429\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.10572399477549786 -0.10572399986868856\n",
            "0.0 0.0 4.5799639071258856e-69\n",
            "0.0 0.0 -2.589832351696845e-95\n",
            "0.0 0.0 1.657491642534164e-53\n",
            "0.0 -2.428026668610795e-06 -2.4280241449292187e-06\n",
            "0.0 0.0 6.371598374114917e-42\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.006784362357460019 -0.00678434473369342\n",
            "0.0 0.0 2.1442343012401276e-69\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.1773039588437637 0.17730447161664195\n",
            "0.0 0.0 4.017219101282587e-62\n",
            "0.0 0.0 0.0\n",
            "0.0 2.9360341935461455e-05 2.9358949809033387e-05\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.24885320230149507 0.24885320159745106\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.66929131787193 -0.669293334825256\n",
            "0.0 0.0 2.0730362948043464e-68\n",
            "0.0 0.0 -1.1722399065575194e-94\n",
            "0.0 0.0 7.5023305925228205e-53\n",
            "0.0 -1.1124529075702386e-05 -1.1124376867301962e-05\n",
            "0.0 0.0 2.883986632344156e-41\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.08381079214193433 -0.08380898098148519\n",
            "0.0 0.0 9.705481574034261e-69\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.006006477496490299 0.006006477501254117\n",
            "0.0 0.0 1.13042677212734e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 -6.13743500466125e-07 -6.137429962960598e-07\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.00627605815539736 0.006276058151850933\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.018869792206557125 -0.018869792254024676\n",
            "0.0 0.0 5.83342771328665e-70\n",
            "0.0 0.0 -3.298628574266508e-96\n",
            "0.0 0.0 2.111120934171088e-54\n",
            "0.0 -2.9035662763024917e-07 -2.903572653757195e-07\n",
            "0.0 0.0 8.115404244280157e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.00032738408761726897 -0.0003273840580154562\n",
            "0.0 0.0 2.7310773731584782e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.006476684144552538 0.0064766841470814095\n",
            "0.0 0.0 1.0556878092667915e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 -8.383782557076315e-07 -8.3837827268399e-07\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.00724042169442187 0.007240421671683359\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.019981379276722815 -0.019981379329966235\n",
            "0.0 0.0 5.4477465421602576e-70\n",
            "0.0 0.0 -3.080537428860458e-96\n",
            "0.0 0.0 1.9715426905899986e-54\n",
            "0.0 -2.760103257060413e-07 -2.7600995634857893e-07\n",
            "0.0 0.0 7.578848591577647e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0008124143957567227 0.0008124144250658032\n",
            "0.0 0.0 2.5505102741066776e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0007866451201543659 0.0007866451188941227\n",
            "0.0 0.0 1.401355489168009e-64\n",
            "0.0 0.0 0.0\n",
            "0.0 -5.993316953834643e-08 -5.993434067326916e-08\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.000896365975888812 0.0008963659762828931\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.0015675233544511563 -0.0015675233541615511\n",
            "0.0 0.0 7.2315219586198005e-71\n",
            "0.0 0.0 -4.08920897636344e-97\n",
            "0.0 0.0 2.6170920671558036e-55\n",
            "0.0 -3.800626480199717e-08 -3.8003872255210206e-08\n",
            "0.0 0.0 1.0060418483619733e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.00028916916416932455 -0.00028916916462628936\n",
            "0.0 0.0 3.3856331072212523e-71\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.004108547503323576 0.0041085475073572705\n",
            "0.0 0.0 6.679794491796263e-64\n",
            "0.0 0.0 0.0\n",
            "0.0 -6.321165813006487e-08 -6.321024999528964e-08\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.004046307665595441 0.004046307663081944\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.011139017185303307 -0.011139017182146757\n",
            "0.0 0.0 3.447025466942105e-70\n",
            "0.0 0.0 -1.9491896120665728e-96\n",
            "0.0 0.0 1.2474805520108145e-54\n",
            "0.0 -1.8091750320081182e-07 -1.8091892641774442e-07\n",
            "0.0 0.0 4.795466143806898e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.0002587774172724192 -0.00025877740676069833\n",
            "0.0 0.0 1.6138184477754637e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.005271584819777656 0.005271584996932003\n",
            "0.0 0.0 1.1677962369005156e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 7.5723116665935025e-06 7.5723086524705e-06\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.007826698459335342 0.007826698446420194\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.042086088348774114 -0.04208608894456683\n",
            "0.0 0.0 6.026268298849828e-70\n",
            "0.0 0.0 -3.4076741469695327e-96\n",
            "0.0 0.0 2.1809100559593775e-54\n",
            "0.0 -2.9981905846913043e-07 -2.9982029732524277e-07\n",
            "0.0 0.0 8.383682068970549e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.008790922881863752 -0.008790922710921103\n",
            "0.0 0.0 2.821360922684377e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0016956524828694787 0.0016956524829185725\n",
            "0.0 0.0 6.446235174033468e-64\n",
            "0.0 0.0 0.0\n",
            "0.0 -7.7430506451035e-07 -7.743061804587358e-07\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0028725688794309753 0.0028725688802107917\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.006718303038334057 -0.0067183030407410626\n",
            "0.0 0.0 3.3265001009651004e-70\n",
            "0.0 0.0 -1.881036129127182e-96\n",
            "0.0 0.0 1.2038623508884698e-54\n",
            "0.0 -1.6381673795253378e-07 -1.6381699369913503e-07\n",
            "0.0 0.0 4.6277925015385204e-43\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.00021592134302752687 0.00021592134660847748\n",
            "0.0 0.0 1.5573912293217755e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.006991708286775082 0.006991708288396953\n",
            "0.0 0.0 1.4574097101381847e-63\n",
            "0.0 0.0 0.0\n",
            "0.0 -1.6703460836710768e-06 -1.6703463047946808e-06\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.008032203056008488 0.008032203059487927\n",
            "0.0 0.0 0.0\n",
            "0.0 -0.018104411957377883 -0.01810441197748316\n",
            "0.0 0.0 7.520782836964596e-70\n",
            "0.0 0.0 -4.252777335417977e-96\n",
            "0.0 0.0 2.721775749839989e-54\n",
            "0.0 -3.8266945168179185e-07 -3.826692468901685e-07\n",
            "0.0 0.0 1.0462835222927354e-42\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.00034410675531230576 0.0003441068300451648\n",
            "0.0 0.0 3.521058431510103e-70\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.7680430780200045 0.7680764230554579\n",
            "0.0 0.0 1.2985894406648694e-61\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0001958264705947898 0.00019568847345948704\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.8706034617522479 0.8705831890243956\n",
            "0.0 0.0 0.0\n",
            "0.0032 -7.901308779028944 -7.904490450446924\n",
            "0.0 0.0 6.701210348321037e-68\n",
            "0.0 0.0 -3.789333651430121e-94\n",
            "0.0 0.0 2.425171982235529e-52\n",
            "0.0 -3.587563246476972e-05 -3.5870490199139276e-05\n",
            "0.0 0.0 9.322654463677645e-41\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0011 -0.5991707331609414 -0.5981194483935887\n",
            "0.0 0.0 3.1373533460250296e-68\n",
            "0.0 0.0 0.0\n",
            "0.0 0.0 0.0\n",
            "0.0 0.004441734554605992 0.004441734550971678\n",
            "0.0 -0.04375252676159162 -0.043752526772320145\n",
            "0.0 0.03931079221253675 0.03931079222134846\n",
            "0.0 0.0 5.010263642539308e-65\n",
            "0.0 0.0 -5.846475458197984e-64\n",
            "0.0 0.0 5.345449093944051e-64\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 2.765543349881077e-07 2.7655333717715033e-07\n",
            "0.0 -2.424127565348311e-07 -2.4241170966913487e-07\n",
            "0.0 -3.414157845327657e-08 -3.414162750801574e-08\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 0.0007103941634945771 0.0007103941645942058\n",
            "0.0 -0.007997008825145528 -0.007997008823983015\n",
            "0.0 0.0072866146605407274 0.007286614659388811\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 -0.1322086577271321 -0.13220865783317448\n",
            "0.0 -0.053661718798509256 -0.053661718851592655\n",
            "0.0 0.1858703767054975 0.18587037668476716\n",
            "0.0 0.0 2.9329285129141713e-71\n",
            "0.0 0.0 -3.422433552893454e-70\n",
            "0.0 0.0 3.1291407016020364e-70\n",
            "0.0 0.0 8.660125422115029e-98\n",
            "0.0 0.0 -1.0105498203044455e-96\n",
            "0.0 0.0 9.23948566083295e-97\n",
            "0.0 0.0 1.3986829142895471e-55\n",
            "0.0 0.0 -1.6321227451172716e-54\n",
            "0.0 0.0 1.4922544536883167e-54\n",
            "0.0 -0.2626691925944625 -0.2626691927079794\n",
            "0.0 0.04926745299505529 0.049267452967445176\n",
            "0.0 0.21340173978592467 0.21340173974053425\n",
            "0.0 0.0 3.205723140763285e-44\n",
            "0.0 0.0 -3.740757536936031e-43\n",
            "0.0 0.0 3.4201852228597025e-43\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 0.018314936827137158 0.01831493674196216\n",
            "0.0 -0.15173198484097047 -0.15173198489937234\n",
            "0.0 0.13341704815707314 0.1334170481574102\n",
            "0.0 0.0 6.77013696847775e-72\n",
            "0.0 0.0 -7.900071146152972e-71\n",
            "0.0 0.0 7.223057449305196e-71\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.049267370293431964 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n",
            "0.0 -0.26266918565556857 -0.26266918576980125\n",
            "0.0 0.04926737029342513 0.04926737026520629\n",
            "0.0 0.21340181554643364 0.21340181550459494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1312233640.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "- The gradient validation appears successful as for most weights the difference between the numerical and analytical gradients is 0.0 which ensures that the `ann` function is correctly calculating the gradients."
      ],
      "metadata": {
        "id": "IJuNmLCLQ5Uy"
      },
      "id": "IJuNmLCLQ5Uy"
    },
    {
      "cell_type": "markdown",
      "id": "c705517e",
      "metadata": {
        "id": "c705517e"
      },
      "source": [
        "# PART B: Training with PyTorch\n",
        "\n",
        "In the second part of the assignment we will see how we can use PyTorch to train a neural network to classify different garbage items.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import shutil"
      ],
      "metadata": {
        "id": "nn3OreQyjsN9"
      },
      "id": "nn3OreQyjsN9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-0. Helper Functions\n",
        "\n",
        "We will be making use of the following helper functions. If you are using Google Colab, you may change runtime type to GPU to speed up the training process in this part. You will be asked to look\n",
        "at and possibly modify some of these, but you are not expected to understand all of them.\n",
        "\n",
        "You should look at the function names and read the docstrings. If you are curious, come back and explore the code *after* making some progress on the lab."
      ],
      "metadata": {
        "id": "J6rZafTFjz-i"
      },
      "id": "J6rZafTFjz-i"
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Data Loading\n",
        "\n",
        "def get_data_loader(data_dir, batch_size, valid_split=0.2):\n",
        "    \"\"\"\n",
        "    Load the Garbage Classification Dataset, split into training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        data_dir: The root directory of the dataset, with subdirectories for each class.\n",
        "        batch_size: Number of samples per batch.\n",
        "        valid_split: Fraction of the training data to be used for validation.\n",
        "\n",
        "    Returns:\n",
        "        train_loader: Iterable DataLoader for training data.\n",
        "        val_loader: Iterable DataLoader for validation data.\n",
        "        test_loader: Iterable DataLoader for testing data.\n",
        "        classes: List of class names.\n",
        "    \"\"\"\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),  # Resize to 128x128\n",
        "        transforms.ToTensor(),         # Convert to Tensor\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    # Load the full dataset\n",
        "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "    classes = dataset.classes  # Get class names\n",
        "\n",
        "    # Split the dataset\n",
        "    total_size = len(dataset)\n",
        "    test_size = int(0.1 * total_size)  # 10% for testing\n",
        "    valid_size = int(valid_split * (total_size - test_size))  # Valid from remaining\n",
        "    train_size = total_size - test_size - valid_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, classes\n",
        "\n",
        "###############################################################################\n",
        "# Training\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n",
        "\n",
        "def normalize_label(labels):\n",
        "    \"\"\"\n",
        "    Normalize labels for multi-class classification.\n",
        "\n",
        "    Args:\n",
        "        labels: A 1D tensor of scalar class labels.\n",
        "    Returns:\n",
        "        The same labels (already suitable for multi-class classification).\n",
        "    \"\"\"\n",
        "    return labels\n",
        "\n",
        "def evaluate(net, loader, criterion):\n",
        "    \"\"\" Evaluate the network on a given dataset.\n",
        "\n",
        "     Args:\n",
        "         net: PyTorch neural network object.\n",
        "         loader: PyTorch DataLoader for valuation data.\n",
        "         criterion: The loss function.\n",
        "\n",
        "     Returns:\n",
        "         err: Average classification error rate over the validation set.\n",
        "         loss: Average loss value over the validation set.\n",
        "    \"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_samples = 0\n",
        "    net = net.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute classification error\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_err += (predicted != labels).sum().item()\n",
        "            total_loss += loss.item()\n",
        "            total_samples += len(labels)\n",
        "\n",
        "    err = total_err / total_samples\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return err, avg_loss\n",
        "\n",
        "###############################################################################\n",
        "# Training Curve\n",
        "\n",
        "def plot_training_curve(path, return_val_error=False):\n",
        "    \"\"\" Plot training and validation error/loss curves.\n",
        "\n",
        "    Args:\n",
        "        path: Base path for the CSV files containing training logs.\n",
        "        return_val_error: If True, return the final validation error.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    train_err = np.loadtxt(f\"{path}_train_err.csv\")\n",
        "    val_err = np.loadtxt(f\"{path}_val_err.csv\")\n",
        "    train_loss = np.loadtxt(f\"{path}_train_loss.csv\")\n",
        "    val_loss = np.loadtxt(f\"{path}_val_loss.csv\")\n",
        "\n",
        "    if return_val_error:\n",
        "        return val_err[-1]  # Return the final validation error\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    plt.plot(range(1, len(train_err) + 1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1, len(val_err) + 1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BxRdbxqRj5Fz"
      },
      "id": "BxRdbxqRj5Fz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-1. Visualizing the Data\n",
        "\n",
        "We will make use of the truncated version of garbage classification dataset, which consists of\n",
        "color images of various garbage items such as batteries, biological items,\n",
        "cardboards, and more. These images are resized to 128x128 pixels.\n",
        "You can find the full dataset at https://www.kaggle.com/datasets/sudipp/garbage-dataset-9-classes\n",
        "\n",
        "Download the \"garbage_dataset.zip\" file from Quercus and upload it to your colab session storage.\n",
        "Run the provided code to automatically unzip the dataset  and split the dataset into\n",
        "training, validation, and testing sets."
      ],
      "metadata": {
        "id": "KB67O_QanTFr"
      },
      "id": "KB67O_QanTFr"
    },
    {
      "cell_type": "code",
      "source": [
        "_ = !unzip \"/content/garbage_dataset.zip\" -d \"/content/garbage_dataset/\""
      ],
      "metadata": {
        "id": "JauHVFXSzlYB"
      },
      "id": "JauHVFXSzlYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Garbage Classification Dataset\n",
        "data_dir = \"./garbage_dataset/\"  # Path to the dataset directory\n",
        "batch_size = 1  # One image per batch for visualization\n",
        "\n",
        "# Use the get_data_loader function to load the dataset\n",
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    data_dir=data_dir, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "WwJt2fYsn-Dj"
      },
      "id": "WwJt2fYsn-Dj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-1(i) [0.5pt EXPLORATORY]\n",
        "\n",
        "Below is some sample plotting code that visualizes a small batch of images from your training set. Currently, it plots 15 images in a 3  5 grid.\n",
        "\n",
        "\n",
        "Modify the code so that it instead plots 30 images in a 3  10 grid, and displays the ground-truth label above each subplot.\n",
        "You may use the provided code as a starting point."
      ],
      "metadata": {
        "id": "bAerWP0v0pfu"
      },
      "id": "bAerWP0v0pfu"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k = 0\n",
        "for images, labels in train_loader:\n",
        "    # since batch_size = 1, there is only 1 image in `images`\n",
        "    image = images[0]\n",
        "    # place the colour channel at the end, instead of at the beginning\n",
        "    img = np.transpose(image, [1,2,0])\n",
        "    # normalize pixel intensity values to [0, 1]\n",
        "    img = img / 2 + 0.5\n",
        "    plt.subplot(3, 5, k+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "    k += 1\n",
        "    if k > 14:\n",
        "        break"
      ],
      "metadata": {
        "id": "kAqMdEg30Kx0"
      },
      "id": "kAqMdEg30Kx0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-1(ii) [1pt EXPLORATORY]\n",
        "\n",
        "How many training examples do we have for the combined classes?\n",
        "What about validation examples?\n",
        "What about test examples?\n",
        "\n",
        "Plot the distribution of class labels in training/validation/test. Comment on whether the dataset is balanced or imbalanced."
      ],
      "metadata": {
        "id": "cMTtJC6G0wH-"
      },
      "id": "cMTtJC6G0wH-"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LKDQzytb01FO"
      },
      "id": "LKDQzytb01FO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CeMTdtRs09Yn"
      },
      "id": "CeMTdtRs09Yn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-2. Training\n",
        "\n",
        "We define two neural networks, a `LargeNet` and `SmallNet`.\n",
        "We'll be training the networks in this section.\n",
        "\n",
        "You won't understand fully what these networks are doing until\n",
        "the next few classes, and that's okay. For this assignment, please\n",
        "focus on learning how to train networks, and how hyperparameters affect\n",
        "training."
      ],
      "metadata": {
        "id": "5YeDqX5X1IIO"
      },
      "id": "5YeDqX5X1IIO"
    },
    {
      "cell_type": "code",
      "source": [
        "class LargeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeNet, self).__init__()\n",
        "        self.name = \"large\"\n",
        "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
        "        self.fc1 = nn.Linear(10 * 29 * 29, 32)\n",
        "        self.fc2 = nn.Linear(32, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 10 * 29 * 29)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ],
      "metadata": {
        "id": "75w8-kEg1K37"
      },
      "id": "75w8-kEg1K37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallNet, self).__init__()\n",
        "        self.name = \"small\"\n",
        "        self.conv = nn.Conv2d(3, 5, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(5 * 31 * 31, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 5 * 31 * 31)\n",
        "        x = self.fc(x)\n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ],
      "metadata": {
        "id": "txDxK6321MXL"
      },
      "id": "txDxK6321MXL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_net = SmallNet()\n",
        "large_net = LargeNet()"
      ],
      "metadata": {
        "id": "mAtjPyAK1N_a"
      },
      "id": "mAtjPyAK1N_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(i) [1pt EXPLORATORY]\n",
        "\n",
        "The methods `small_net.parameters()` and `large_net.parameters()`\n",
        "produces an iterator of all the trainable parameters of the network.\n",
        "These parameters are torch tensors containing many scalar values.\n",
        "\n",
        "We haven't learned how how the parameters in these high-dimensional\n",
        "tensors will be used, but we should be able to count the number\n",
        "of parameters. Measuring the number of parameters in a network is\n",
        "one way of measuring the \"size\" of a network.\n",
        "\n",
        "What is the total number of parameters in `small_net` and in\n",
        "`large_net`? (Hint: how many numbers are in each tensor?)"
      ],
      "metadata": {
        "id": "7cE8ntVM1Pt3"
      },
      "id": "7cE8ntVM1Pt3"
    },
    {
      "cell_type": "code",
      "source": [
        "for param in small_net.parameters():\n",
        "    print(param.shape)"
      ],
      "metadata": {
        "id": "OuNISsjj1Tf9"
      },
      "id": "OuNISsjj1Tf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IpfGfw_21Uuq"
      },
      "id": "IpfGfw_21Uuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "QUh4Ywll1VHs"
      },
      "id": "QUh4Ywll1VHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The function train_net\n",
        "\n",
        "The function `train_net` below takes an untrained neural network (like `small_net` and `large_net`) and\n",
        "several other parameters. You should be able to understand how this function works.\n",
        "The figure below shows the high level training loop for a machine learning model:\n",
        "\n",
        "![alt text](https://github.com/UTNeural/Lab2/blob/master/Diagram.png?raw=true)"
      ],
      "metadata": {
        "id": "sytMp0l61ZF9"
      },
      "id": "sytMp0l61ZF9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory in Drive to save everything\n",
        "save_dir = \"/content/drive/MyDrive/garbage_model_runs\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "ECzGn75WPs8x"
      },
      "id": "ECzGn75WPs8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30, save_dir=save_dir):\n",
        "    # Moving model to GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    net = net.to(device)\n",
        "    ########################################################################\n",
        "    # Train a classifier on an 7-class classification problem\n",
        "    target_classes = [\"battery\", \"biological\", \"cardboard\", \"glass\",\n",
        "                      \"metal\", \"paper\", \"plastic\"]\n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible results\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "            data_dir='/content/garbage_dataset',  # Ensure this is where your dataset is stored\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Make sure the classes are as expected\n",
        "    assert len(classes) == 7, \"The dataset should have exactly 7 classes.\"\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # Use CrossEntropyLoss for multi-class classification\n",
        "    criterion = nn.CrossEntropyLoss()  # Change to CrossEntropyLoss for multi-class\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/accuracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # Get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)  # No need to normalize labels here for CrossEntropyLoss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Calculate the statistics\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest probability\n",
        "            total_train_err += (predicted != labels).sum().item()  # Count errors\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "\n",
        "        # Calculate error and loss for the training and validation set\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i + 1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "\n",
        "        # Print stats for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: Train err: {train_err[epoch]:.4f}, Train loss: {train_loss[epoch]:.4f} | \"\n",
        "              f\"Validation err: {val_err[epoch]:.4f}, Validation loss: {val_loss[epoch]:.4f}\")\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = os.path.join(\n",
        "            save_dir,\n",
        "            get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        )\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Total time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Write the train/test loss/error into CSV files for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(f\"{model_path}_train_err.csv\", train_err)\n",
        "    np.savetxt(f\"{model_path}_train_loss.csv\", train_loss)\n",
        "    np.savetxt(f\"{model_path}_val_err.csv\", val_err)\n",
        "    np.savetxt(f\"{model_path}_val_loss.csv\", val_loss)\n"
      ],
      "metadata": {
        "id": "7VhMyIjO1Yk6"
      },
      "id": "7VhMyIjO1Yk6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(ii) [0.5pt EXPLORATORY]\n",
        "\n",
        "The parameters to the function `train_net` are hyperparameters of our neural network.\n",
        "We made these hyperparameters easy to modify so that we can tune them later on.\n",
        "\n",
        "What are the default values of the parameters `batch_size`, `learning_rate`,\n",
        "and `num_epochs`?"
      ],
      "metadata": {
        "id": "hVqzPMXk4XOU"
      },
      "id": "hVqzPMXk4XOU"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "iQLY1Y2C4THF"
      },
      "id": "iQLY1Y2C4THF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(iii) [0.5pt EXPLORATORY]\n",
        "What files are written to disk when we call `train_net` with `small_net`, and train for 5 epochs? Provide a list\n",
        "of all the files written to disk, and what information the files contain."
      ],
      "metadata": {
        "id": "I5NKyPrO7LvQ"
      },
      "id": "I5NKyPrO7LvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "YrNntjUu7Ppr"
      },
      "id": "YrNntjUu7Ppr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(iv) [0.5pt EXPLORATORY]\n",
        "Train both `small_net` and `large_net` using the function `train_net` and its default parameters.\n",
        "The function will write many files to disk, including a model checkpoint (saved values of model weights)\n",
        "at the end of each epoch.\n",
        "\n",
        "If you are using Google Colab, you will need to mount Google Drive\n",
        "so that the files generated by `train_net` gets saved. We will be using\n",
        "these files in the parts that follow.\n",
        "(See the Google Colab tutorial for more information about this.)\n",
        "\n",
        "Report the total time elapsed when training each network. Which network took longer to train?\n",
        "Why?"
      ],
      "metadata": {
        "id": "DZRoK4ok7UMR"
      },
      "id": "DZRoK4ok7UMR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the function writes files to disk, you will need to mount\n",
        "# your Google Drive to save model runs later. If you are working on the assignment locally, you\n",
        "# can comment out this code.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i3y41Pxf7aLi"
      },
      "id": "i3y41Pxf7aLi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PKqKY0FO7eYk"
      },
      "id": "PKqKY0FO7eYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "55H-R0u-7fow"
      },
      "id": "55H-R0u-7fow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(v) [0.5pt EXPLORATORY]\n",
        "Use the function `plot_training_curve` to display the trajectory of the\n",
        "training/validation error and the training/validation loss.\n",
        "You will need to use the function `get_model_name` to generate the\n",
        "argument to the `plot_training_curve` function.\n",
        "\n",
        "Do this for both the small network and the large network. Include both plots\n",
        "in your writeup."
      ],
      "metadata": {
        "id": "09dZDLkW7jFz"
      },
      "id": "09dZDLkW7jFz"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Qk71Vrp7nKj"
      },
      "id": "7Qk71Vrp7nKj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "1cVgtZX87q5-"
      },
      "id": "1cVgtZX87q5-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-2(vi) [0.5pt EXPLORATORY]\n",
        "Describe what you notice about the training curve.\n",
        "How do the curves differ for `small_net` and `large_net`?\n",
        "Identify any occurences of underfitting and overfitting."
      ],
      "metadata": {
        "id": "GLENCJFm7tmT"
      },
      "id": "GLENCJFm7tmT"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "RsosA_1-7sWr"
      },
      "id": "RsosA_1-7sWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-3. Optimization Parameters\n",
        "\n",
        "For this section, we will work with `large_net` only."
      ],
      "metadata": {
        "id": "w-81PxcS7x1S"
      },
      "id": "w-81PxcS7x1S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-3(i) [0.5pt EXPLORATORY]\n",
        "Train `large_net` with all default parameters, except set `learning_rate=0.001`.\n",
        "Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *lowering* the learning rate."
      ],
      "metadata": {
        "id": "CO7a2Ezp72yl"
      },
      "id": "CO7a2Ezp72yl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: When we re-construct the model, we start the training\n",
        "# with *random weights*. If we omit this code, the values of\n",
        "# the weights will still be the previously trained values.\n",
        "large_net = LargeNet()"
      ],
      "metadata": {
        "id": "1tfX_JWp78Nq"
      },
      "id": "1tfX_JWp78Nq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZpVa4VKz79ca"
      },
      "id": "ZpVa4VKz79ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "C816NbA67-kv"
      },
      "id": "C816NbA67-kv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-3(ii) [0.5pt EXPLORATORY]\n",
        "Train `large_net` with all default parameters, except set `learning_rate=0.1`.\n",
        "Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *increasing* the learning rate."
      ],
      "metadata": {
        "id": "FdEQ3w7R8AI6"
      },
      "id": "FdEQ3w7R8AI6"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSnl3QMI8CkF"
      },
      "id": "zSnl3QMI8CkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "VFYCCS2g8D94"
      },
      "id": "VFYCCS2g8D94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-3(iii) [0.5pt EXPLORATORY]\n",
        "\n",
        "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
        "Now, set `batch_size=512`. Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *increasing* the batch size."
      ],
      "metadata": {
        "id": "lFRCTrUp8F88"
      },
      "id": "lFRCTrUp8F88"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJoQPisk8FVZ"
      },
      "id": "aJoQPisk8FVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_6PJAMF78J26"
      },
      "id": "_6PJAMF78J26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-3(iv) [0.5pt EXPLORATORY]\n",
        "\n",
        "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
        "Now, set `batch_size=16`. Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *decreasing* the batch size."
      ],
      "metadata": {
        "id": "1BnA9CuR8Npu"
      },
      "id": "1BnA9CuR8Npu"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NEjU4IIh8M27"
      },
      "id": "NEjU4IIh8M27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "W_HlhE6r8Rle"
      },
      "id": "W_HlhE6r8Rle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-4. Hyperparameter Search"
      ],
      "metadata": {
        "id": "P5Eh2-L-8SKX"
      },
      "id": "P5Eh2-L-8SKX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-4(i) [0.5pt EXPLORATORY]\n",
        "\n",
        "Based on the plots from above, choose another set of values for the hyperparameters (network, batch_size, learning_rate)\n",
        "that you think would help you improve the validation accuracy. Justify your choice."
      ],
      "metadata": {
        "id": "aT4W7BC48VEU"
      },
      "id": "aT4W7BC48VEU"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "dzkSDUEd8Xg-"
      },
      "id": "dzkSDUEd8Xg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-4(ii) [0.5pt EXPLORATORY]\n",
        "\n",
        "Train the model with the hyperparameters you chose in part(i), and include the training curve."
      ],
      "metadata": {
        "id": "vnzH_beq8cn3"
      },
      "id": "vnzH_beq8cn3"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "swNlC6uW8b90"
      },
      "id": "swNlC6uW8b90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-4(iii) [0.5pt EXPLORATORY]\n",
        "Based on your result from Part(i), suggest another set of hyperparameter values to try.\n",
        "Justify your choice."
      ],
      "metadata": {
        "id": "Jxt9iRoR8glc"
      },
      "id": "Jxt9iRoR8glc"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "RolEnE2586ak"
      },
      "id": "RolEnE2586ak",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-4(iv) [0.5pt EXPLORATORY]\n",
        "\n",
        "Train the model with the hyperparameters you chose in part(iii), and include the training curve."
      ],
      "metadata": {
        "id": "8V9OjC6N88Hk"
      },
      "id": "8V9OjC6N88Hk"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0wZFrYBW8-aY"
      },
      "id": "0wZFrYBW8-aY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-4(v) [1pt EXPLORATORY]\n",
        "\n",
        "\n",
        "Using the models you trained in earlier subparts of Part B-4, plot a heatmap of validation errors across at least two hyperparameters (e.g., batch size vs. learning rate). Make sure to select at least two values for each hyperparameter and train the model for all combinations. The rows correspond to different batch sizes, the columns correspond to different learning rates, and the cell values are the validation errors.\n",
        "\n",
        "Briefly explain any trends you observe and what they suggest about the effect of these hyperparameters.\n",
        "\n",
        "\n",
        " Hint: A heatmap is simply a color-coded grid where each cells color represents a numerical value. For example, heres a small heatmap created with random values:\n",
        "\n",
        "  `random_vals = np.random.rand(3, 4)`\n",
        "\n",
        "  `sns.heatmap(random_vals, annot=True, fmt=\".2f\")`\n",
        "  \n",
        "  `plt.title(\"Example Heatmap with Random Values\")`\n",
        "  \n",
        "  `plt.show()`\n",
        "\n"
      ],
      "metadata": {
        "id": "gqs-Y3c1HXuN"
      },
      "id": "gqs-Y3c1HXuN"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EVlIsdqHHV66"
      },
      "id": "EVlIsdqHHV66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "GvU1OcOaHXOx"
      },
      "id": "GvU1OcOaHXOx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-5. Evaluating the Best Model\n"
      ],
      "metadata": {
        "id": "edbCWzvY9AY2"
      },
      "id": "edbCWzvY9AY2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(i) [2pt MODEL]\n",
        "\n",
        "Summarize in a table the results achieved on training and validation on all your model selections in all parts. This includes choice of `small_net` vs `large_net`, the `batch_size`, `learning_rate`,\n",
        "**and the epoch number**. Hint: you can import the Pandas module to create tables.\n",
        "\n",
        "\n",
        "Then upon reviewing the results choose your **best** model and load the model checkpoint. You can modify the code below to load your chosen set of weights to the model object `net`."
      ],
      "metadata": {
        "id": "-sGKFKhg9CbC"
      },
      "id": "-sGKFKhg9CbC"
    },
    {
      "cell_type": "code",
      "source": [
        "net = #small or large network\n",
        "model_path = os.path.join(save_dir, get_model_name(net.name, batch_size=64, learning_rate=0.01, epoch=10))\n",
        "state = torch.load(model_path)\n",
        "net.load_state_dict(state)"
      ],
      "metadata": {
        "id": "PXyhQtVk8_9E"
      },
      "id": "PXyhQtVk8_9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sTLheeu49G_b"
      },
      "id": "sTLheeu49G_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(ii) [4pt DISCUSSION]\n",
        "\n",
        "Justify your choice of model from Part (i)."
      ],
      "metadata": {
        "id": "ZzJXQgy99Jv0"
      },
      "id": "ZzJXQgy99Jv0"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "5BhaFEZa9M5y"
      },
      "id": "5BhaFEZa9M5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(iii) [2pt RESULT]\n",
        "\n",
        "Using the helper code in Part 0, any code from lecture notes, or any code that you write:\n",
        "\n",
        "- Compute and report the **test classification error** for your chosen model.\n",
        "\n",
        "\n",
        "- For 5 test images, print the true label of the image, and report the top-3 predicted classes with their probabilities.\n",
        "(Top-3 means the three classes with the highest predicted probabilities, showing the models first, second, and third best guesses.)\n",
        "Comment on whether the probabilities reflect the models confidence (e.g., does it give a very high probability to a wrong class, or spread probabilities when uncertain?).\n"
      ],
      "metadata": {
        "id": "Uqa03V0b9OYA"
      },
      "id": "Uqa03V0b9OYA"
    },
    {
      "cell_type": "code",
      "source": [
        "# If you use the `evaluate` function provided in part 0, you will need to\n",
        "# set batch_size > 1\n",
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    data_dir=data_dir,\n",
        "    batch_size=64)"
      ],
      "metadata": {
        "id": "Vdf_sjGO9RtL"
      },
      "id": "Vdf_sjGO9RtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fSHlK5t9TYJ"
      },
      "id": "3fSHlK5t9TYJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(iv) [3pt DISCUSSION]\n",
        "\n",
        "How does the test classification error compare with the **validation error**?\n",
        "Explain why you would expect the test error to be *higher* than the validation error."
      ],
      "metadata": {
        "id": "Ewm-UEJl9Vui"
      },
      "id": "Ewm-UEJl9Vui"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "fcUUN4VM9U3E"
      },
      "id": "fcUUN4VM9U3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(v) [3pt DISCUSSION]\n",
        "Why did we only use the test data set at the very end?\n",
        "Why is it important that we use the test data as little as possible?"
      ],
      "metadata": {
        "id": "saYTvJA99ZVP"
      },
      "id": "saYTvJA99ZVP"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "-ghsE4Vc9cW2"
      },
      "id": "-ghsE4Vc9cW2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part B-5(vi) [3pt RESULT]\n",
        "Preprocess the images from the secret test set provided below and classify these images into the 7 class labels from the garbage dataset using your chosen model. Report the model predictions by your chosen model on this test dataset as a csv file called 'labels_part_b5.csv', containing the name of the images and their corresponsing predicted labels for all the images in the folder."
      ],
      "metadata": {
        "id": "2gCrqkB49eqy"
      },
      "id": "2gCrqkB49eqy"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "-OalQTUz-BjD"
      },
      "id": "-OalQTUz-BjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "myfile = wget.download('https://github.com/Sabaae/Dataset/blob/main/secret_testset.zip')"
      ],
      "metadata": {
        "id": "G3Efr-yH-C3s"
      },
      "id": "G3Efr-yH-C3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oKonFXSL-Fq8"
      },
      "id": "oKonFXSL-Fq8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B-6. Fully-Connected Linear ANN vs CNN [4pt RESULT]\n",
        "\n",
        "Test out a 3-layer linear fully-connected ANN architecture (see simpleANN below). You should explore different hyperparameter settings to determine how well you can do on the validation dataset. Once satisified with the performance, you may test it out on the test data.\n",
        "\n",
        "\n",
        "Similar to what you did in the previous part, report the model predictions by your best ANN architecture on the secret test dataset as a csv file called 'labels_part_b6.csv', containing the names of the images and their corresopnding predicted labels for all the images in the folder.\n",
        "\n",
        "\n",
        "How does the your best CNN model compare with an 2-layer linear ANN model (no convolutional layers) on classifying garbage dataset images?\n"
      ],
      "metadata": {
        "id": "fyTmeZ45-HBP"
      },
      "id": "fyTmeZ45-HBP"
    },
    {
      "cell_type": "code",
      "source": [
        "class simpleANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(simpleANN, self).__init__()\n",
        "        self.name = \"simple\"\n",
        "        self.fc1 = nn.Linear(128*128*3, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 128*128*3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MgNauOoa-K1t"
      },
      "id": "MgNauOoa-K1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XEJQthE9-MN3"
      },
      "id": "XEJQthE9-MN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "coqGuP7V-Nmk"
      },
      "id": "coqGuP7V-Nmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART C (Optional) - Bonus Challenge!\n",
        "\n",
        "This is an optional exercise for those that finish the assignment early and would like to perform a deeper exploration of the assignment.\n",
        "\n",
        "In part A we constructed and trained a 2-layer neural network from scratch. In Part B we saw how PyTorch can be used to simplify the construction of neural networks by taking care of all the complexity related to gradient calculations, training on GPUs, and structuring your code.\n",
        "\n",
        "For this bonus challenge we will propose additional task that will have you work towards completing the pipeline and deploying the models online, while exploring ways to improve these models along the way.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Modify your code to classify images that are not \"paper\" or \"cardboard\" into the category \"other\".\n",
        "2. Preprocess images from the internet to be able to classify any images into garbage labels such as battery, biological, etc. (i.e., resize, aspect ratio, etc.). Evaluate your best model's performance on images loaded from the internet.\n",
        "3. Deploy your best model onto huggingface spaces (or other web hosting services) to classify images into the different garbage classes.\n",
        "\n",
        "Bonus marks will be provided based on the number of tasks completed and how well they are completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade."
      ],
      "metadata": {
        "id": "b9tDxneb-KVD"
      },
      "id": "b9tDxneb-KVD"
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jbi-pPaH-R3c"
      },
      "id": "Jbi-pPaH-R3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "4EUd_JMf-TEA"
      },
      "id": "4EUd_JMf-TEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving to HTML\n",
        "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
        "\n",
        "(1) download your ipynb file by clicking on File->Download.ipynb\n",
        "\n",
        "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
        "\n",
        "(3) run the following:"
      ],
      "metadata": {
        "id": "tKSEjquu-gpq"
      },
      "id": "tKSEjquu-gpq"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html A1.ipynb"
      ],
      "metadata": {
        "id": "edqA_4jS-Ua-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354b694a-0cd4-4610-d886-d3e111a6f4a7"
      },
      "id": "edqA_4jS-Ua-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook A1.ipynb to html\n",
            "[NbConvertApp] Writing 433116 bytes to A1.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(4) the html file will be available for download in the temporary Google Colab storage\n",
        "\n",
        "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
      ],
      "metadata": {
        "id": "U1XQPoOw-j12"
      },
      "id": "U1XQPoOw-j12"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Grading Rubric\n",
        "The grading of the assignment will be based on the following categories:\n",
        "\n",
        "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
        "\n",
        "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
        "\n",
        "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
        "\n",
        "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
        "\n",
        "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
        "\n",
        "(6) **5 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
        "\n",
        "\n",
        "\n",
        "**TOTAL GRADE = _____ of 50 Pts**"
      ],
      "metadata": {
        "id": "3HlqCol_-lbx"
      },
      "id": "3HlqCol_-lbx"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}